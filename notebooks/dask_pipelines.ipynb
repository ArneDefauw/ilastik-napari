{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_image.imread import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter_list = (\n",
    "#    filters.Gaussian,\n",
    "#    filters.LaplacianOfGaussian,\n",
    "#    filters.GaussianGradientMagnitude,\n",
    "#    filters.DifferenceOfGaussians,\n",
    "#    filters.StructureTensorEigenvalues,\n",
    "#   filters.HessianOfGaussianEigenvalues,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "import dask.array as da\n",
    "from dask_image.ndfilters import (\n",
    "    gaussian_filter,\n",
    "    gaussian_gradient_magnitude,\n",
    "    gaussian_laplace,\n",
    "    laplace,\n",
    ")\n",
    "\n",
    "from ilastik.napari.filters import GaussianDask, FilterSet\n",
    "\n",
    "features=FilterSet( filters=[GaussianDask( scale=0.3 ),GaussianDask( scale=0.7 ) ] ) # -> we use dask filters\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from spatialdata import read_zarr\n",
    "\n",
    "dask.config.set({'distributed.worker.daemon': False})\n",
    "\n",
    "path = \"/Users/arnedf/VIB/DATA/test_data_ilastik/output\" # change this\n",
    "\n",
    "sdata = read_zarr(\"/Users/arnedf/VIB/DATA/test_data_ilastik/sdata_multi_channel.zarr\")\n",
    "\n",
    "#image=imread( \"/Users/arnedf/VIB/DATA/test_data_ilastik/fov0/*.tiff\" )\n",
    "\n",
    "image=sdata[ \"raw_image\" ].data\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "imshow( image[0] )  # we only plot the first channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "def preprocessing_dask(image, estimators, preprocessing_path=None):\n",
    "    pipe = Pipeline(estimators)\n",
    "    feature_map_lazy = pipe.transform(image)\n",
    "    feature_map_lazy.to_zarr( os.path.join( preprocessing_path , \"array.zarr\") ) # this could be large, so we write to zarr store\n",
    "    joblib.dump(pipe, os.path.join( preprocessing_path, \"preprocessing_pipe.pkl\" ))\n",
    "\n",
    "features=FilterSet( filters=[GaussianDask( scale=0.3 ),GaussianDask( scale=0.7 ) ] )\n",
    "estimators = [(\"features\", features)]\n",
    "\n",
    "preprocessing_path = path\n",
    "\n",
    "preprocessing_dask( image[0], estimators=estimators, preprocessing_path = preprocessing_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_image=da.from_zarr( os.path.join( preprocessing_path, \"array.zarr\" ) )\n",
    "\n",
    "preprocessed_image # preprocessed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some dummy annotations\n",
    "\n",
    "labels = np.random.choice([0, 1, 2], size=image.shape[1:], p=[0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "from ilastik.napari.classifier import NDSparseDaskClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import loguru\n",
    "\n",
    "logger = loguru.logger\n",
    "\n",
    "def pixel_training_dask(\n",
    "    X,labels, model_path=None, **client_kwargs,\n",
    "):\n",
    "    clf = NDSparseDaskClassifier(RandomForestClassifier(n_jobs=-1))\n",
    "    # add the classifier to the pipe, and then dump it\n",
    "    client = Client(**client_kwargs)\n",
    "    logger.info(f\"Client dashboard link {client.dashboard_link}\")\n",
    "\n",
    "    with joblib.parallel_backend(\n",
    "        \"dask\"\n",
    "    ):  # note, NDSparseDaskClassifier with dask backend will still load data that was annotated in memory (although not the full dataset, only non-zero labels)\n",
    "        clf.fit(X, labels)\n",
    "\n",
    "    if model_path is not None:\n",
    "        joblib.dump(clf, os.path.join(model_path))\n",
    "\n",
    "# load features from the zarr store\n",
    "image =  da.from_zarr( os.path.join( preprocessing_path, \"array.zarr\" ) )\n",
    "pixel_training_dask( X=image, labels=labels, model_path=os.path.join( path, \"model.pkl\" ), n_workers=1, threads_per_worker=10  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_classification_dask(\n",
    "    image: da.Array | None,\n",
    "    preprocessing_path,\n",
    "    model_path,\n",
    "    tmp_path,\n",
    "    **client_kwargs,\n",
    "):\n",
    "    # WIP\n",
    "    if image is None:\n",
    "        # case where we train and run inference on same image\n",
    "        image = da.from_zarr( os.path.join( preprocessing_path,  \"array.zarr\" ) )\n",
    "    else:\n",
    "        # load the preprocessing pipe from the path, do the preprocessing on image, and then do the classification\n",
    "        # this should be used if we have a new image coming in, that we want to preprocesses and classify using pretrained model and the same preprocessing pipe.\n",
    "        preprocessing_pipe = joblib.load(preprocessing_path / \"pipe.pkl\")\n",
    "        image = preprocessing_pipe.transform(image)\n",
    "        # image could be large\n",
    "        image.to_zarr(tmp_path)\n",
    "        image=da.from_zarr(tmp_path)\n",
    "    clf = joblib.load(model_path)\n",
    "    client = Client(**client_kwargs)\n",
    "\n",
    "    clf_scatter = client.scatter(\n",
    "        clf\n",
    "    )  # scatter the model otherwise issues with large task graph\n",
    "\n",
    "    def _predict_clf(arr, model):\n",
    "        arr = model.predict(arr)\n",
    "        return arr.squeeze(-1)\n",
    "\n",
    "    # probably need to use map_overlap instead of map_blocks here\n",
    "    array_result = da.map_blocks(\n",
    "        _predict_clf,\n",
    "        image,\n",
    "        dtype=image.dtype,\n",
    "        drop_axis=-1,\n",
    "        chunks=image.chunks[:-1],\n",
    "        model=clf_scatter,\n",
    "        # TODO output dtype not correct, need to fix via meta\n",
    "    )\n",
    "\n",
    "    results = array_result.compute()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pixel_classification_dask(image = None, preprocessing_path=path, model_path=os.path.join( path, \"model.pkl\" ), tmp_path = None,  n_workers=1, threads_per_worker=10  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results # ->predicted labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilastik_napari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
